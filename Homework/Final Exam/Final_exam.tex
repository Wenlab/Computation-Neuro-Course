\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\title{Final Exam of Computational Neuroscience}

\date{Due January 31, 2021}

\begin{document}
\maketitle

\section*{Poisson Spike-Train Statistics}
1. Given the Homogeneous Poisson process (mean firing rate is independent of time), 
\begin{equation}
P(n)= \frac{(rT)^n}{n!}\exp(-rT).
\end{equation}
Calculate the mean $\langle n \rangle$ and variance $Var(n)$ of the spike count. Compute the Fano factor $Var(n)/\langle n \rangle$. 
Calculate the kurtosis of spike count defined as $k = \langle n^4 \rangle - 3 \langle n^2 \rangle^2$ in the time interval $T$. 
\\
\\
2. When the firing rate depends on time, we could also extend the homogeneous Poisson process to inhomogeneous Poisson process. When $n$ spikes occurs in an interval $T$ with $0<t_1<t_2<...<t_n<T$,  Prove that  the joint probability density is given by 
\begin{equation}
p(t_1,t_2,...,t_n)=\exp\left(-\int_0^Tr(t)dt\right)\prod_{i=1}^{n}r(t_i)
\end{equation}
Then, calculate the probability of seeing $n$ spikes $P(n)$ in the interval $T$. Check whether it has a similar expression as the homogeneous Poisson process and calculate the Fano Factor.
\\
\\
3. Generate a Poisson spike train with a time-dependent fire rate $r(t) = r_0[1 + cos(2\pi t/\tau)]$ where $r_0$ = 100 Hz and $\tau = 300$ ms. Generate a spike train for 20 s and plot it.
\\
\\
4*. Let's assume that the firing rate of a neuron has the following functional form: $r(t)=r_0+r_1\sin(\omega t+\theta)$, where the phase $\theta$  is drawn uniformly between $0$ and $2\pi$ for each trial. Calculate the Fano Factor for the spike count in the time interval $T$ (as a function of $T$).
\\
\\
Note: Problems with * are optional. However, solving them will give you additional credits.


\section*{Sensory Neuron from an Electric Fish}
Electric fish can generate and sense electric fields. The response of a electrosensory neuron $R(t)$ is characterized by the firing rate, which is the number of spikes (action potential)occurred within a time window divided by the time bin size $\Delta t$.  Use the following equation 
\begin{equation}
R(t) = R_0 + \int_0^\infty D(\tau) s(t-\tau) dt,
\end{equation}
with $R_0 = 50$ Hz, and 
\begin{equation}
D(\tau) = -\cos (\frac{2\pi (\tau - 20 \mathrm{ ms})}{140 \mathrm{ ms}} )\exp ( - \frac{\tau}{60 \mathrm{ ms}} ) \mathrm{ Hz/ms},
\end{equation}
to predict the response of a neuron of the electrosensory lateral-line lobe to a stimulus. Use an approximate Gaussian white noise stimulus constructed by choosing a stimulus value every 10 ms ($\Delta t = 10$ ms) from a Gaussian distribution with zero mean and variance $\sigma^2 /\Delta t$, with $\sigma^2 =10$. A detailed description of white noise can be found on page 21 of the theoretical neuroscience textbook. 
\begin{enumerate}
\item[1.]Compute the firing rate over a 10 s period. 
\item[2.]From the results, compute the firing rate-stimulus correlation function $Q_{rs}(\tau)$. 
\item[3.]Compare $Q_{rs}(-\tau)/\sigma^2$ with the kernel $D(\tau)$ given above.
\end{enumerate}

\section*{Winner Take All Circuit}

Consider the following recurrent network dynamics of $N$ neurons:

\begin{eqnarray}
\frac{dx_{i}}{dt} &=&-x_{i}+\left[ b_{i}+\alpha x_{i}-\beta \sum_{j=1,j\neq
i}^{N}x_{j}\right] _{+} \\
&=&-x_{i}+\left[ b_{i}+(Wx)_{i}\right] _{+}  \label{eq:wta}
\end{eqnarray}

where we have denoted $W=(\alpha +\beta )I-\beta $\b{1}$_{N}$ \ (\b{1}$_{N}$
is an $N\times N$ matrix of ones) and  $\left[ x\right] _{+}\equiv max(x,0)$. \ $%
\alpha >0$ is self-excitation and $\beta >0$ represents a global
inhibition.

\bigskip The external inputs ($b_{i}>0$) are fixed in time and we assume
that they are all different from each other. Prove the following:

\begin{enumerate}
\item[*A.] If $\alpha <1$, then the network will converge asympotatically to
a fixed point from almost all initial conditions.

\item[B.] If $\alpha <1$ and $\beta >1-\alpha $ then the only possible
stable states of the network are fixed points in which only a single neuron
is active.

\item[C.] Given B, the neurons that can remain active at long time are those
for which:
\begin{equation}
b_{i}\geq \frac{1-\alpha }{\beta }b_{\max }  \label{active}
\end{equation}%
(where $b_{\max }=\max_{i}b_{i}$). From this result, derive the
conditions which guarantee that, independent of the initial
conditions, the network evolves into a state where only the neuron
with the largest $b_{i}$ is active (i.e., the network picks the
"winner" neuron).
\end{enumerate}
\bigskip
\textbf{Instructions for *A: }Prove that
\begin{eqnarray}
E(x) &=&-\sum_{i}b_{i}x_{i}+\frac{1}{2}(1-\alpha )\sum_{i}x_{i}^{2}+\frac{%
\beta }{2}\sum_{i\not=j}x_{i}x_{j}  \label{eqn:Lyapunov} \\
&=&\frac{1}{2}x^{T}x-b^{T}x-\frac{1}{2}x^{T}Wx
\end{eqnarray}%
is a Lyapunov function of the system. When calculating $\frac{dE}{dt}$ it is
useful to consider separately the contributions from neurons such that $%
b_{i}+(Wx)_{i}>0$ and those for which $b_{i}+(Wx)_{i}<0$.
\\
\\
\textbf{Instructions for B: }Assume there exists a fixed point with
$K$ active neurons. We can arrange the order of the neurons in the
system so that: $x_{1}^{\ast },...x_{K}^{\ast }>0$,  $x_{K+1}^{\ast
},...x_{N}^{\ast }=0$ where $K$ denotes the number of active neurons
in the fixed point$.$ By linearizing around such a fixed point, prove that if $%
\alpha <1$ and $\beta >1-\alpha $, then the fixed point is stable
iff $K=1$. Note: stability of the inactive neurons is quite
straightforward to show. For the active neurons you need to
diagonalize a matrix of the form \b{1}$_{K}$. \ If you have
difficulty in this part, try at least to analyze the stability of
the state with $K=1$.
\\
\\
\textbf{Instructions for C: \ }Assume a fixed point with a single active
neuron as claimed by \textbf{B }and show that consistency requires that the
active neuron obey the 
property Equation \ref{active}.
\\
\\
\textbf{Note: }If you have difficulty with proving A, you can assume A and 
proceed to prove B and C. Likewise, if you have difficulties proving B,
assume that  B holds and proceed to prove C.


\end{document}